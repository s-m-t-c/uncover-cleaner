{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 0,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Uncover ML target preprocessoor\n",
    "#### This notebook exists to perform statistical preanalysis on input geochemistry target files for the UncoverML project\n",
    "* Import a .csv file containing geochemistry data with LAT LON\n",
    "* Process negative detection limits to useable data\n",
    "* Group the data by duplicated measurements taken at each location\n",
    "* Calculate the median of those groups and replace each group with the median value in the original dataframe\n",
    "* Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import and header / value standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:08:22.623462Z",
     "start_time": "2018-11-06T03:08:14.928715Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from numbers import Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T03:49:39.005846Z",
     "start_time": "2018-08-06T03:49:22.032467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in geochemistry data\n",
    "\n",
    "ozchem = pd.read_csv(\"champ_chem_rock_rego_geo_mapV3.csv\", header=0, low_memory=False)\n",
    "nt = pd.read_csv(\"nt_gchem_v2.csv\", header=0, low_memory=False)\n",
    "qld = pd.read_csv(\"qld_reprojected.csv\", header=0, low_memory=False)\n",
    "wa = pd.read_csv(\"wa_gchem_v2.1.csv\", header=0, low_memory=False)\n",
    "nsw = pd.read_csv(\"nsw_shifted.csv\", header=0, low_memory=False)\n",
    "sa = pd.read_csv(\"sa_gchem_v2.csv\", header=0, low_memory=False)\n",
    "sa_calc = pd.read_csv(\"calcrete_geology_SA_v1.csv\", header=0, low_memory=False)\n",
    "sa_rock = pd.read_csv(\"SA_rock_ppm.csv\", header=0, low_memory=False)\n",
    "\n",
    "regolith = pd.read_csv(\"regolithmaster.csv\", header=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T03:49:39.019207Z",
     "start_time": "2018-08-06T03:49:39.008579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign a name identifier before merge\n",
    "\n",
    "ozchem['name'] = 'ozchem'\n",
    "nt['name'] = 'nt'\n",
    "qld['name'] = 'qld'\n",
    "wa['name'] = 'wa'\n",
    "nsw['name'] = 'nsw'\n",
    "sa['name'] = 'sa'\n",
    "sa_calc['name'] = 'sa_calc'\n",
    "sa_rock['name'] = 'sa_rock'\n",
    "regolith['name'] = 'regolith'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T03:49:39.043713Z",
     "start_time": "2018-08-06T03:49:39.021624Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These print statements exist to show all the elements in each input dataset to create the element list\n",
    "\n",
    "# print(list(ozchem))\n",
    "# print(list(nt))\n",
    "# print(list(qld))\n",
    "# print(list(wa))\n",
    "# print(list(nsw))\n",
    "# print(list(sa))\n",
    "# print(list(sa_calc))\n",
    "# print(list(sa_rock))\n",
    "# print(list(regolith))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T03:49:39.110913Z",
     "start_time": "2018-08-06T03:49:39.064008Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the oxides in the sa_rock and sa_calc datasets and add them back into their respective dataframes\n",
    "\n",
    "def al_oxide(al):\n",
    "    return al/0.5293\n",
    "\n",
    "def ca_oxide(ca):\n",
    "    return ca/0.7147\n",
    "\n",
    "def k_oxide(k):\n",
    "    return k/0.8301\n",
    "\n",
    "def mg_oxide(mg):\n",
    "    return mg/0.603\n",
    "\n",
    "def na_oxide(na):\n",
    "    return na/0.7419\n",
    "\n",
    "def si_oxide(si):\n",
    "    return si/0.4674\n",
    "\n",
    "def fe_oxide(fe):\n",
    "    return fe/0.6994\n",
    "\n",
    "sa_rock_elements = ['Fe__', 'Mg__', 'Ca__', 'Si__', 'Na__', 'K__', 'Al__']\n",
    "sa_calc_elements = ['Ca_pct', 'Fe_pct','Mg_pct','K_pct']\n",
    "\n",
    "sa_calc['fe2o3_pct'] = np.vectorize(fe_oxide)(sa_calc['Fe_pct'])\n",
    "sa_calc['cao_pct'] = np.vectorize(ca_oxide)(sa_calc['Ca_pct'])\n",
    "sa_calc['k2o_pct'] = np.vectorize(k_oxide)(sa_calc['K_pct'])\n",
    "sa_calc['mgo_pct'] = np.vectorize(mg_oxide)(sa_calc['Mg_pct'])\n",
    "sa_rock['fe2o3_pct'] = np.vectorize(fe_oxide)(sa_rock['Fe__']/10000)\n",
    "sa_rock['cao_pct'] = np.vectorize(ca_oxide)(sa_rock['Ca__']/10000)\n",
    "sa_rock['k2o_pct'] = np.vectorize(k_oxide)(sa_rock['K__']/10000)\n",
    "sa_rock['mgo_pct'] = np.vectorize(mg_oxide)(sa_rock['Mg__']/10000)\n",
    "sa_rock['sio2_pct'] = np.vectorize(ca_oxide)(sa_rock['Si__']/10000)\n",
    "sa_rock['na2o_pct'] = np.vectorize(k_oxide)(sa_rock['Na__']/10000)\n",
    "sa_rock['al2o3_pct'] = np.vectorize(mg_oxide)(sa_rock['Al__']/10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 11,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "**Modify the column headings of the dataframe for consistency**\n",
    "\n",
    "**Create a generic list of elements by which to conduct summary statistics on**\n",
    "\n",
    "**Calculate the intersection of columns and element lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T03:49:54.838868Z",
     "start_time": "2018-08-06T03:49:54.642371Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the inputs for the merge loop\n",
    "\n",
    "inputs = [ozchem, nt, qld, wa, nsw, sa, sa_calc, sa_rock, regolith]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T03:50:04.951388Z",
     "start_time": "2018-08-06T03:49:55.210251Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "national_gchem = pd.DataFrame(columns=[], dtype=float)\n",
    "\n",
    "for df in inputs:\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    df = df.replace([-9999, -999, 99999], np.nan)\n",
    "    columns = list(df.columns)\n",
    "    location = ['lon', 'long', 'lat', 'longitude', 'latitude', 'longda94', 'latgda94', 'dlat','dlong']\n",
    "    location_in_columns = list(set.intersection(set(location), set(columns)))\n",
    "    fe2o3 = ['fe2o3_pct','fe2o3', 'fe2o3_pc']\n",
    "    fe2o3t = ['fe2o3t_pct', 'fe2o3t', 'fe2o3tot']\n",
    "    feo = ['feo_pct','feo', 'feo_pc']\n",
    "    al = ['al2o3', 'al2o3_pc', 'al2o3_pct']\n",
    "    si = ['sio2', 'sio2_pct', 'sio2_pc']\n",
    "    ca = ['cao', 'cao_pct', 'cao_pc']\n",
    "    mg = ['mgo', 'mgo_pct', 'mgo_pc']\n",
    "    na = ['na2o', 'na2o_pct', 'na2o_pc']\n",
    "    k = ['k2o', 'k2o_pct', 'k2o_pc']\n",
    "    ti = ['tio2', 'tio2_pct', 'tio2_pc']\n",
    "    mn = ['mno', 'mno_pct', 'mno_pc']\n",
    "    p = ['p2o5', 'p2o5_pct', 'p2o5_pc']\n",
    "    fe2o3t_in_columns = list(set.intersection(set(fe2o3t), set(columns)))\n",
    "    fe2o3_in_columns = list(set.intersection(set(fe2o3), set(columns)))\n",
    "    feo_in_columns = list(set.intersection(set(feo), set(columns)))\n",
    "    al_in_columns = list(set.intersection(set(al), set(columns)))\n",
    "    si_in_columns = list(set.intersection(set(si), set(columns)))\n",
    "    ca_in_columns = list(set.intersection(set(ca), set(columns)))\n",
    "    mg_in_columns = list(set.intersection(set(mg), set(columns)))\n",
    "    na_in_columns = list(set.intersection(set(na), set(columns)))\n",
    "    k_in_columns = list(set.intersection(set(k), set(columns)))\n",
    "    ti_in_columns = list(set.intersection(set(ti), set(columns)))\n",
    "    mn_in_columns = list(set.intersection(set(mn), set(columns)))\n",
    "    p_in_columns = list(set.intersection(set(p), set(columns)))\n",
    "    keep = location_in_columns + fe2o3_in_columns + feo_in_columns + fe2o3t_in_columns + al_in_columns + si_in_columns + ca_in_columns + mg_in_columns + na_in_columns + k_in_columns + ti_in_columns + p_in_columns + mn_in_columns + ['name'] + ['sample_no'] + ['sampleid'] + ['sample_id'] + [\n",
    "        'lithology'] + ['reliability'] + ['lithname'] + ['lith_group'] + ['rock_type'] + ['collected_date'] + ['extraction_date'] + ['send_date'] + ['collected1']\n",
    "    df = df.filter(keep)\n",
    "    df.rename(columns={'long': 'longitude','lon': 'longitude', 'lat': 'latitude', 'longda94': 'longitude',\n",
    "                       'latgda94': 'latitude','dlat': 'latitude', 'dlong': 'longitude', 'extraction_date': 'date', 'collected_date':'date', 'collected1': 'date', 'send_date':'date', 'fe2o3': 'fe2o3_pct', 'fe2o3_pc':'fe2o3_pct','feo':'feo_pct','feo_pc': 'feo_pct', 'fe2o3t':'fe2o3t_pct', 'fe2o3tot':'fe2o3t_pct', 'al2o3': 'al2o3_pct', 'al2o3_pc':'al2o3_pct', 'sio2': 'sio2_pct', 'sio2_pc': 'sio2_pct', 'cao' : 'cao_pct', 'cao_pc' : 'cao_pct', 'mgo': 'mgo_pct', 'mgo_pc':'mgo_pct', 'na2o': 'na2o_pct', 'na2o_pc':'na2o_pct', 'k2o':'k2o_pct', 'k2o_pc':'k2o_pct', 'tio2': 'tio2_pct', 'tio2_pc':'tio2_pct', 'mno':'mno_pct', 'mno_pc':'mno_pct', 'p2o5': 'p2o5_pct', 'p2o5_pc':'p2o5_pct', 'sampleid': 'sample_id'}, inplace =True)\n",
    "    df.sort_index(axis=1, inplace=True)\n",
    "    national_gchem = pd.concat([national_gchem, df], axis=0)\n",
    "    print(df.name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:06.394260Z",
     "start_time": "2018-11-06T03:21:06.386256Z"
    }
   },
   "outputs": [],
   "source": [
    "# set list of elements to be present in the final dataset\n",
    "\n",
    "elements = ['al2o3_pct','feo_pct','fe2o3_pct', 'fe2o3t_pct', 'sio2_pct', 'mgo_pct', 'cao_pct', 'na2o_pct', 'k2o_pct', 'tio2_pct', 'p2o5_pct', 'mno_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T03:50:24.260633Z",
     "start_time": "2018-08-06T03:50:22.354716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coerce all the elements and the location data to numeric type\n",
    "\n",
    "national_gchem[elements] = national_gchem[elements].apply(pd.to_numeric, errors='coerce')\n",
    "national_gchem[['latitude', 'longitude']] = national_gchem[['latitude', 'longitude']].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where all elements have a value of NaN or 0\n",
    "print('number of observations:', national_gchem.shape[0])\n",
    "national_gchem.dropna(subset = elements, how='all', inplace=True)\n",
    "print('number of observations with NaN rows removed:', national_gchem.shape[0])\n",
    "national_gchem = national_gchem[~(national_gchem[elements] == 0).all(axis = 1)]\n",
    "print('number of observations with 0 rows removed:', national_gchem.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate, outlier and detection limit processing of geochemistry measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T03:50:31.732089Z",
     "start_time": "2018-08-06T03:50:31.067890Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert detection limits in a set range to useful vales by dividing by -2\n",
    "# Convert values with 0 to nan and those less than -2.5 (the lower detection limit) to nan\n",
    "\n",
    "def detectionlimits(value):\n",
    "    if -2.5 < value < 0:\n",
    "        modified_value = value / -2\n",
    "        return modified_value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "vdetectionlimits = np.vectorize(detectionlimits)\n",
    "\n",
    "for column in national_gchem[elements]:\n",
    "    national_gchem.loc[national_gchem[column] <= -2.5, column] = np.nan\n",
    "    national_gchem.loc[national_gchem[column] == 0, column] = np.nan\n",
    "\n",
    "\n",
    "national_gchem[elements] = national_gchem[elements].apply(vdetectionlimits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (Re)Calculate total iron under 'fe2o3t_pct\n",
    "\n",
    "# Function to check whether a number is a float or integer and not a NaN.\n",
    "\n",
    "def numberchecker(value):\n",
    "    if isinstance(value, Number) and pd.isnull(value) == False and value != 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Keep the existing fe2o3t measurement if it's not similar to either of its inputs\n",
    "# Calculate the total iron if both fe2o3 and feo are numbers    \n",
    "\n",
    "def totalfe(fe2o3, feo, fe2o3t):\n",
    "    if  numberchecker(fe2o3t) and not (fe2o3-.02) < fe2o3t < (fe2o3+.02) and not (feo-.02) < fe2o3t < (feo+.02):\n",
    "        return fe2o3t\n",
    "    elif numberchecker(fe2o3) and numberchecker(feo):\n",
    "        return fe2o3 + feo*1.11134\n",
    "    elif numberchecker(fe2o3):\n",
    "        return fe2o3\n",
    "    elif numberchecker(feo):\n",
    "        return feo*1.11134\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "national_gchem['fe2o3t_pct'] = np.vectorize(totalfe)(national_gchem['fe2o3_pct'], national_gchem['feo_pct'], national_gchem['fe2o3t_pct'])\n",
    "elements = ['al2o3_pct','feo_pct', 'fe2o3t_pct','fe2o3_pct', 'sio2_pct', 'mgo_pct', 'cao_pct', 'na2o_pct', 'k2o_pct', 'tio2_pct', 'p2o5_pct', 'mno_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T03:51:34.687460Z",
     "start_time": "2018-08-06T03:51:34.457782Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate median of duplicate measurements at locations\n",
    "# Recalculate the list of elements incase empty columns were dropped in the groupby\n",
    "\n",
    "national_gchem_dupemedian = national_gchem.groupby(['longitude', 'latitude'], as_index=False)[elements].agg('median')\n",
    "columns = list(national_gchem_dupemedian.columns)\n",
    "elements = list(set.intersection(set(elements), set(columns)))\n",
    "print('shape before calculating median of duplicates at locations:', national_gchem.shape)\n",
    "print('shape after calculating median of duplicates at locations:', national_gchem_dupemedian.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T03:51:53.493014Z",
     "start_time": "2018-08-06T03:51:53.166056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make observations of samples with over 100% or over the 99th percentile of a single element equal to NaN\n",
    "\n",
    "national_gchem_dupemedian_outlier = national_gchem_dupemedian.copy()\n",
    "\n",
    "for column in national_gchem_dupemedian_outlier[elements]:\n",
    "    max = np.nanpercentile(national_gchem_dupemedian_outlier[column], 99)\n",
    "    national_gchem_dupemedian_outlier.loc[national_gchem_dupemedian_outlier[column] > 100, column] = np.nan\n",
    "    national_gchem_dupemedian_outlier.loc[national_gchem_dupemedian_outlier[column] > max, column] = np.nan\n",
    "    \n",
    "# Drop rows where all values are equal to NaN (removing the outliers defined above')\n",
    "\n",
    "print('Before outliers removed:', national_gchem_dupemedian_outlier.shape)\n",
    "national_gchem_dupemedian_outlier.dropna(subset = elements, inplace=True, how='all')\n",
    "print('After outliers removed:', national_gchem_dupemedian_outlier.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T00:39:18.069860Z",
     "start_time": "2018-08-06T00:39:18.055631Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmerge(left,right,**kwargs):\n",
    "\n",
    "    # Function to flatten lists from http://rosettacode.org/wiki/Flatten_a_list#Python\n",
    "    def flatten(lst):\n",
    "        return sum( ([x] if not isinstance(x, list) else flatten(x) for x in lst), [] )\n",
    "    \n",
    "    # Set default for removing overlapping columns in \"left\" to be true\n",
    "    myargs = {'replace':'left'}\n",
    "    myargs.update(kwargs)\n",
    "    \n",
    "    # Remove the replace key from the argument dict to be sent to\n",
    "    # pandas merge command\n",
    "    kwargs = {k:v for k,v in myargs.items() if k is not 'replace'}\n",
    "    \n",
    "    if myargs['replace'] is not None:\n",
    "        # Generate a list of overlapping column names not associated with the join\n",
    "        skipcols = set(flatten([v for k, v in myargs.items() if k in ['on','left_on','right_on']]))\n",
    "        leftcols = set(left.columns)\n",
    "        rightcols = set(right.columns)\n",
    "        dropcols = list((leftcols & rightcols).difference(skipcols))\n",
    "        \n",
    "        # Remove the overlapping column names from the appropriate DataFrame\n",
    "        if myargs['replace'].lower() == 'left':\n",
    "            left = left.copy().drop(dropcols,axis=1)\n",
    "        elif myargs['replace'].lower() == 'right':\n",
    "            right = right.copy().drop(dropcols,axis=1)\n",
    "        \n",
    "    df = pd.merge(left,right,**kwargs)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 15,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "**Drop the duplicates from the original dataset,  keeping the first observation at a location**\n",
    "\n",
    "**Fill the geochemistry values in the above dataset with the different levels of processed data whilst keeping the metadata using the defined rmerge function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# drop duplicates based on lat and lon\n",
    "national_gchem_dupe_drop = national_gchem.drop_duplicates(\n",
    "    ['longitude', 'latitude'], keep='first'\n",
    ")\n",
    "\n",
    "# replace the geochemistry values in the duplicate dropped dataset with the declim_dupemedian values\n",
    "national_gchem_dupemedian = rmerge(\n",
    "    national_gchem_dupe_drop, national_gchem_dupemedian, on=['longitude','latitude']\n",
    ")\n",
    "\n",
    "# add metadata back to duplicate median dataset\n",
    "national_gchem_dupemedian_outlier = rmerge(\n",
    "    national_gchem_dupe_drop, national_gchem_dupemedian_outlier, on=['longitude','latitude']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(national_gchem.shape)\n",
    "print(national_gchem_dupe_drop.shape)\n",
    "print(national_gchem_dupemedian.shape)\n",
    "print(national_gchem_dupemedian_outlier.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the year from the input data and add is as a column\n",
    "\n",
    "def yearfinder(year):\n",
    "    if pd.isnull(year):\n",
    "        return year\n",
    "    x = re.findall('[\\d]{4}', year)\n",
    "    #result = [int(x) for x in x]\n",
    "    return x\n",
    "\n",
    "national_gchem_dupemedian_outlier['year'] = national_gchem_dupemedian_outlier['date'].apply(yearfinder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:20:45.805012Z",
     "start_time": "2018-11-06T03:20:45.794829Z"
    }
   },
   "outputs": [],
   "source": [
    "'''This list is needed to drop duplicate entries based on their geochemistry values \n",
    "excluding total iron because of slight variations in it's calculation'''\n",
    "\n",
    "elements_no_fe2o3t = ['al2o3_pct','feo_pct','fe2o3_pct', 'sio2_pct', 'mgo_pct', 'cao_pct', 'na2o_pct', 'k2o_pct', 'tio2_pct', 'p2o5_pct', 'mno_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples that contain the index pairs of rows with identical elements\n",
    "\n",
    "# element_duplicates = national_gchem_dupe_drop[national_gchem_dupe_drop[elements_no_fe2o3t].duplicated(keep=False)]\n",
    "# element_duplicates = element_duplicates.groupby(element_duplicates[elements_no_fe2o3t].columns.tolist()).apply(lambda x: tuple(x.index)).tolist()\n",
    "# element_duplicates = pd.DataFrame(element_duplicates, columns = ['element_duplicate_index_1', 'element_duplicate_index_2', 'element_duplicate_index_3', 'element_duplicate_index_4'])\n",
    "# element_duplicates.drop(columns = ['element_duplicate_index_3', 'element_duplicate_index_4'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_gchem_dupemedian_outlier['FID'] = range(0, len(national_gchem_dupemedian_outlier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# national_gchem_dupemedian_outlier = pd.merge(\n",
    "#     national_gchem_dupemedian_outlier, element_duplicates, how='left', left_on = 'FID', right_on = 'element_duplicate_index_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "national_gchem_dupemedian_outlier.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_gchem_dupemedian_outlier.to_csv('final.csv', sep = ',', index=False)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:08:23.344320Z",
     "start_time": "2018-11-06T03:08:22.946005Z"
    }
   },
   "outputs": [],
   "source": [
    "national_gchem_dupemedian_outlier = pd.read_csv('test.csv', header=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:08:35.862554Z",
     "start_time": "2018-11-06T03:08:32.713217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import geopandas as gpd\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.geometry import Point\n",
    "from fiona.crs import from_epsg\n",
    "sys.path.append('/g/data/u46/users/sc0554/anaconda3/envs/work')\n",
    "sys.path.append('/g/data/u46/users/sc0554/anaconda3/lib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataframe to a geodataframe by extracting the geometry from the Lat and Lon and projecting it to Albers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:08:57.182258Z",
     "start_time": "2018-11-06T03:08:49.090445Z"
    }
   },
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(national_gchem_dupemedian_outlier.longitude, \n",
    "                                    national_gchem_dupemedian_outlier.latitude)]\n",
    "\n",
    "#national_gchem_declim_dupemedian_outlier_elementdrop = national_gchem_declim_dupemedian_outlier_elementdrop.drop(['longitude', 'latitude'], axis=1)\n",
    "crs = {'init': 'epsg:4326'}\n",
    "national_gchem_shape = gpd.GeoDataFrame(national_gchem_dupemedian_outlier, crs=crs, geometry=geometry)\n",
    "national_gchem_shape.drop(columns=['latitude', 'longitude'], inplace=True)\n",
    "national_gchem_shape['geometry'] = national_gchem_shape['geometry'].to_crs(epsg = 3577)\n",
    "national_gchem_shape.crs = from_epsg(3577)\n",
    "national_gchem_shape.crs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add geometry as x, y columns in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:20:17.517563Z",
     "start_time": "2018-11-06T03:20:11.966825Z"
    }
   },
   "outputs": [],
   "source": [
    "def getXY(pt):\n",
    "    return (pt.x, pt.y)\n",
    "centroidseries = national_gchem_shape['geometry'].centroid\n",
    "x,y = [list(t) for t in zip(*map(getXY, centroidseries))]\n",
    "national_gchem_shape['x'] = x\n",
    "national_gchem_shape['y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculate the spatial distance between the observations with element duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:20:17.530898Z",
     "start_time": "2018-11-06T03:20:17.520721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function that creates a list of tuples that contain the index pairs of rows with identical elements\n",
    "\n",
    "def duplicate_distance_calculator(df, dupe_list):\n",
    "    index_1_list = []\n",
    "    index_2_list = []\n",
    "    fid_1_list = []\n",
    "    fid_2_list = []\n",
    "    distances = []\n",
    "    #mark all duplicates using the input list as True\n",
    "    duplicates = df[df[dupe_list].duplicated(keep=False)]\n",
    "    #conduct a groupby on just the duplicates to find the indexes of each duplicate pair\n",
    "    duplicates = duplicates.groupby(duplicates[dupe_list].columns.tolist()).apply(lambda x: tuple(x.index)).tolist()\n",
    "    for duplicate_pair in duplicates:\n",
    "        index_1 = duplicate_pair[0]\n",
    "        index_2 = duplicate_pair[1]\n",
    "        duplicate_1_location = df.iloc[index_1].geometry\n",
    "        duplicate_1_fid = df.iloc[index_1].FID\n",
    "        duplicate_2_location = df.iloc[index_2].geometry\n",
    "        duplicate_2_fid = df.iloc[index_2].FID\n",
    "        #calculate the distance between the two points that are duplicates\n",
    "        distance = duplicate_1_location.distance(duplicate_2_location)\n",
    "        #append this to a list\n",
    "        fid_1_list.append(duplicate_1_fid)\n",
    "        fid_2_list.append(duplicate_2_fid)\n",
    "        index_1_list.append(index_1)\n",
    "        index_2_list.append(index_2)\n",
    "        distances.append(distance)\n",
    "    distance_indexes = pd.DataFrame({'index_1': fid_1_list, 'index_2': fid_2_list, 'geochem_dupe_distance': distances})\n",
    "    return distance_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:17.193243Z",
     "start_time": "2018-11-06T03:21:16.898830Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "distanceindexes = duplicate_distance_calculator(national_gchem_shape, elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:20.043505Z",
     "start_time": "2018-11-06T03:21:19.726817Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the duplicate indexes and distances to the main geodataframe\n",
    "\n",
    "national_gchem_shape_distances = pd.merge(national_gchem_shape, distanceindexes, how='left', left_on='FID', right_on='index_1')\n",
    "#national_gchem_shape_distances = pd.merge(national_gchem_shape_distances, distanceindexes, how='left', left_on='FID', right_on='index_2')\n",
    "national_gchem_shape_distances.update(distanceindexes[['index_1', 'index_2', 'geochem_dupe_distance']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:20.721098Z",
     "start_time": "2018-11-06T03:21:20.712240Z"
    }
   },
   "outputs": [],
   "source": [
    "# national_gchem_shape_distances = pd.read_csv('nat_gchem_distances2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:21.000898Z",
     "start_time": "2018-11-06T03:21:20.944001Z"
    }
   },
   "outputs": [],
   "source": [
    "national_gchem_shape_distances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:24.039643Z",
     "start_time": "2018-11-06T03:21:23.857035Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a list of the FIDs to drop from the dataframe based on their distance, preferencing ozchem\n",
    "indexes_to_drop = []\n",
    "for duplicates in distanceindexes.itertuples():\n",
    "    if duplicates.geochem_dupe_distance < 40:\n",
    "        index_1 = duplicates.index_1\n",
    "        index_2 = duplicates.index_2\n",
    "        duplicate_1 = national_gchem_shape_distances[national_gchem_shape_distances['FID'] == index_1]\n",
    "        duplicate_2 = national_gchem_shape_distances[national_gchem_shape_distances['FID'] == index_2]\n",
    "        if duplicate_1.name.item() != 'ozchem':\n",
    "            indexes_to_drop.append(duplicate_1.FID.item())\n",
    "        if duplicate_2.name.item() !='ozchem':\n",
    "            indexes_to_drop.append(duplicate_2.FID.item())\n",
    "        else:\n",
    "            indexes_to_drop.append(duplicate_1.FID.item())\n",
    "    if duplicates.geochem_dupe_distance > 40:\n",
    "        indexes_to_drop.append(duplicates.index_1)\n",
    "        indexes_to_drop.append(duplicates.index_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:26.092824Z",
     "start_time": "2018-11-06T03:21:26.063791Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "national_gchem_shape_distances_dropped = national_gchem_shape_distances.loc[\n",
    "    ~national_gchem_shape_distances['FID'].isin(indexes_to_drop)]\n",
    "\n",
    "print(national_gchem_shape_distances.shape)\n",
    "print(national_gchem_shape_distances_dropped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:26.940688Z",
     "start_time": "2018-11-06T03:21:26.919030Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('# of observations', national_gchem_shape_distances.shape[0])\n",
    "print('# of observations where the duplicate element pairs are greater than 40 meters apart:', national_gchem_shape_distances.loc[\n",
    "    national_gchem_shape_distances.geochem_dupe_distance > 40].shape[0])\n",
    "print('# of observations where the duplicate element pairs are less than 40 meters apart:', national_gchem_shape_distances.loc[\n",
    "    national_gchem_shape_distances.geochem_dupe_distance < 40].shape[0])\n",
    "print('# of observations after dropping:', national_gchem_shape_distances_dropped.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:27.790074Z",
     "start_time": "2018-11-06T03:21:27.782623Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('precision', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:30.885300Z",
     "start_time": "2018-11-06T03:21:30.847895Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Preprocessed elements\")\n",
    "national_gchem[elements].round(1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:36.886737Z",
     "start_time": "2018-11-06T03:21:36.727843Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Post-processed elements\")\n",
    "national_gchem_shape_distances[elements].round(1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:21:53.606310Z",
     "start_time": "2018-11-06T03:21:52.770273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot of the frequency of distances between duplicate pairs\n",
    "\n",
    "distanceindexes['geochem_dupe_distance'].loc[distanceindexes['geochem_dupe_distance'] < 100].plot(kind= 'hist', bins=20, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:22:12.779635Z",
     "start_time": "2018-11-06T03:22:12.757358Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_elements = ['al2o3_pct','feo_pct','fe2o3_pct', 'mgo_pct', 'cao_pct', 'sio2_pct', 'na2o_pct', 'k2o_pct', 'tio2_pct', 'p2o5_pct', 'mno_pct']\n",
    "national_gchem[plot_elements].plot(kind = 'box', vert=False, figsize =(20,5), xlim=(0,100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:22:13.860770Z",
     "start_time": "2018-11-06T03:22:13.061394Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "national_gchem_shape_distances[plot_elements].plot(kind = 'box', vert=False, figsize =(20,5), xlim=(0,100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_gchem_shape_distances.to_csv('nat_gchem_distances_final.csv', sep = ',', index=False)\n",
    "\n",
    "# national_gchem_shape_distances_dropped.to_csv('nat_gchem_distances_dropped_final.csv', sep = ',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T03:31:08.745167Z",
     "start_time": "2018-11-06T03:28:21.496551Z"
    }
   },
   "outputs": [],
   "source": [
    "national_gchem_shape_distances.to_file('national_gchem.shp')\n",
    "national_gchem_shape_distances_dropped.to_file('national_gchem_dropped.shp')"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
